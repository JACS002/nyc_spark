{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a7fafe5-9e38-4252-9b8f-fac2b64a417c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting snowflake-snowpark-python[pandas]\n",
      "  Downloading snowflake_snowpark_python-1.40.0-py3-none-any.whl.metadata (170 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.6/170.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=40.6.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-snowpark-python[pandas]) (68.2.2)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.11/site-packages (from snowflake-snowpark-python[pandas]) (0.41.2)\n",
      "Collecting snowflake-connector-python<4.0.0,>=3.17.0 (from snowflake-snowpark-python[pandas])\n",
      "  Downloading snowflake_connector_python-3.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (74 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.8/74.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions<5.0.0,>=4.1.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-snowpark-python[pandas]) (4.8.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from snowflake-snowpark-python[pandas]) (6.0.1)\n",
      "Requirement already satisfied: cloudpickle!=2.1.0,!=2.2.0,<=3.1.1,>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-snowpark-python[pandas]) (3.0.0)\n",
      "Requirement already satisfied: protobuf<6.32,>=3.20 in /opt/conda/lib/python3.11/site-packages (from snowflake-snowpark-python[pandas]) (4.24.3)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.11/site-packages (from snowflake-snowpark-python[pandas]) (2.8.2)\n",
      "Collecting tzlocal (from snowflake-snowpark-python[pandas])\n",
      "  Downloading tzlocal-5.3.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting asn1crypto<2.0.0,>0.24.0 (from snowflake-connector-python<4.0.0,>=3.17.0->snowflake-snowpark-python[pandas])\n",
      "  Downloading asn1crypto-1.5.1-py2.py3-none-any.whl.metadata (13 kB)\n",
      "Collecting boto3>=1.24 (from snowflake-connector-python<4.0.0,>=3.17.0->snowflake-snowpark-python[pandas])\n",
      "  Downloading boto3-1.40.53-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting botocore>=1.24 (from snowflake-connector-python<4.0.0,>=3.17.0->snowflake-snowpark-python[pandas])\n",
      "  Downloading botocore-1.40.53-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: cffi<2.0.0,>=1.9 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python<4.0.0,>=3.17.0->snowflake-snowpark-python[pandas]) (1.16.0)\n",
      "Requirement already satisfied: cryptography>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python<4.0.0,>=3.17.0->snowflake-snowpark-python[pandas]) (41.0.4)\n",
      "Requirement already satisfied: pyOpenSSL<26.0.0,>=22.0.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python<4.0.0,>=3.17.0->snowflake-snowpark-python[pandas]) (23.2.0)\n",
      "Requirement already satisfied: pyjwt<3.0.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python<4.0.0,>=3.17.0->snowflake-snowpark-python[pandas]) (2.8.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python<4.0.0,>=3.17.0->snowflake-snowpark-python[pandas]) (2023.3.post1)\n",
      "Requirement already satisfied: requests<3.0.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python<4.0.0,>=3.17.0->snowflake-snowpark-python[pandas]) (2.31.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python<4.0.0,>=3.17.0->snowflake-snowpark-python[pandas]) (23.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python<4.0.0,>=3.17.0->snowflake-snowpark-python[pandas]) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python<4.0.0,>=3.17.0->snowflake-snowpark-python[pandas]) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python<4.0.0,>=3.17.0->snowflake-snowpark-python[pandas]) (2023.7.22)\n",
      "Collecting filelock<4,>=3.5 (from snowflake-connector-python<4.0.0,>=3.17.0->snowflake-snowpark-python[pandas])\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: sortedcontainers>=2.4.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python<4.0.0,>=3.17.0->snowflake-snowpark-python[pandas]) (2.4.0)\n",
      "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python<4.0.0,>=3.17.0->snowflake-snowpark-python[pandas]) (3.11.0)\n",
      "Collecting tomlkit (from snowflake-connector-python<4.0.0,>=3.17.0->snowflake-snowpark-python[pandas])\n",
      "  Downloading tomlkit-0.13.3-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting pandas<3.0.0,>=2.1.2 (from snowflake-connector-python[pandas]<4.0.0,>=3.17.0; extra == \"pandas\"->snowflake-snowpark-python[pandas])\n",
      "  Downloading pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyarrow in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python[pandas]<4.0.0,>=3.17.0; extra == \"pandas\"->snowflake-snowpark-python[pandas]) (13.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil->snowflake-snowpark-python[pandas]) (1.16.0)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.24->snowflake-connector-python<4.0.0,>=3.17.0->snowflake-snowpark-python[pandas])\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.15.0,>=0.14.0 (from boto3>=1.24->snowflake-connector-python<4.0.0,>=3.17.0->snowflake-snowpark-python[pandas])\n",
      "  Downloading s3transfer-0.14.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.11/site-packages (from botocore>=1.24->snowflake-connector-python<4.0.0,>=3.17.0->snowflake-snowpark-python[pandas]) (2.0.7)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.11/site-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python<4.0.0,>=3.17.0->snowflake-snowpark-python[pandas]) (2.21)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/conda/lib/python3.11/site-packages (from pandas<3.0.0,>=2.1.2->snowflake-connector-python[pandas]<4.0.0,>=3.17.0; extra == \"pandas\"->snowflake-snowpark-python[pandas]) (1.24.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas<3.0.0,>=2.1.2->snowflake-connector-python[pandas]<4.0.0,>=3.17.0; extra == \"pandas\"->snowflake-snowpark-python[pandas]) (2023.3)\n",
      "Downloading snowflake_connector_python-3.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading snowflake_snowpark_python-1.40.0-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tzlocal-5.3.1-py3-none-any.whl (18 kB)\n",
      "Downloading asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading boto3-1.40.53-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading botocore-1.40.53-py3-none-any.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Downloading pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tomlkit-0.13.3-py3-none-any.whl (38 kB)\n",
      "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading s3transfer-0.14.0-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: asn1crypto, tzlocal, tomlkit, jmespath, filelock, pandas, botocore, s3transfer, boto3, snowflake-connector-python, snowflake-snowpark-python\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.0.3\n",
      "    Uninstalling pandas-2.0.3:\n",
      "      Successfully uninstalled pandas-2.0.3\n",
      "Successfully installed asn1crypto-1.5.1 boto3-1.40.53 botocore-1.40.53 filelock-3.20.0 jmespath-1.0.1 pandas-2.3.3 s3transfer-0.14.0 snowflake-connector-python-3.18.0 snowflake-snowpark-python-1.40.0 tomlkit-0.13.3 tzlocal-5.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"snowflake-snowpark-python[pandas]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e2fcfff-c4e3-417d-954c-6d9c233d0726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Conectado: NYC_TAXI_DM.RAW  (WH=COMPUTE_WH, ROLE=ACCOUNTADMIN)\n",
      "✅ Objetos listos:\n",
      "   Tabla RAW: NYC_TAXI_DM.RAW.TRIPS_RAW\n",
      "   Stage: @NYC_TAXI_DM.RAW.RAW_STAGE\n",
      "   File Format: NYC_TAXI_DM.RAW.FF_PARQUET_TRIPS\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "from snowflake.snowpark import Session\n",
    "\n",
    "# --- Conexión desde variables de entorno (.env en Docker) ---\n",
    "cfg = {\n",
    "    \"account\":   os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n",
    "    \"user\":      os.getenv(\"SNOWFLAKE_USER\"),\n",
    "    \"password\":  os.getenv(\"SNOWFLAKE_PASSWORD\"),\n",
    "    \"role\":      os.getenv(\"SNOWFLAKE_ROLE\", \"SYSADMIN\"),\n",
    "    \"warehouse\": os.getenv(\"SNOWFLAKE_WAREHOUSE\"),\n",
    "    \"database\":  os.getenv(\"SNOWFLAKE_DATABASE\"),\n",
    "    \"schema\":    os.getenv(\"SNOWFLAKE_SCHEMA_RAW\", \"RAW\"),\n",
    "}\n",
    "\n",
    "session = Session.builder.configs(cfg).create()\n",
    "print(f\"✅ Conectado: {cfg['database']}.{cfg['schema']}  (WH={cfg['warehouse']}, ROLE={cfg['role']})\")\n",
    "\n",
    "# --- Variables globales ---\n",
    "DB, SCHEMA = cfg[\"database\"], cfg[\"schema\"]\n",
    "STAGE  = \"RAW_STAGE\"\n",
    "FF_PQ  = \"FF_PARQUET_TRIPS\"\n",
    "TABLE  = f\"{DB}.{SCHEMA}.TRIPS_RAW\"\n",
    "\n",
    "# --- Limpieza y creación de objetos ---\n",
    "session.sql(f\"CREATE SCHEMA IF NOT EXISTS {DB}.{SCHEMA}\").collect()\n",
    "session.sql(f\"DROP TABLE IF EXISTS {TABLE}\").collect()\n",
    "session.sql(f\"DROP STAGE IF EXISTS {DB}.{SCHEMA}.{STAGE}\").collect()\n",
    "session.sql(f\"DROP FILE FORMAT IF EXISTS {DB}.{SCHEMA}.{FF_PQ}\").collect()\n",
    "\n",
    "# --- Crear stage y file format (PARQUET no necesita opciones para timestamp) ---\n",
    "session.sql(f\"CREATE STAGE {DB}.{SCHEMA}.{STAGE}\").collect()\n",
    "session.sql(f\"CREATE FILE FORMAT {DB}.{SCHEMA}.{FF_PQ} TYPE = PARQUET\").collect()\n",
    "\n",
    "# --- Crear tabla RAW (superset Yellow + Green) ---\n",
    "session.sql(f\"\"\"\n",
    "CREATE TABLE {TABLE} (\n",
    "  SERVICE                STRING,\n",
    "  YEAR                   NUMBER(38,0),\n",
    "  MONTH                  NUMBER(38,0),\n",
    "\n",
    "  VENDORID               NUMBER(38,0),\n",
    "  PICKUP_DATETIME        TIMESTAMP_NTZ,\n",
    "  DROPOFF_DATETIME       TIMESTAMP_NTZ,\n",
    "  PASSENGER_COUNT        NUMBER(38,0),\n",
    "  TRIP_DISTANCE          FLOAT,\n",
    "  RATECODEID             NUMBER(38,0),\n",
    "  STORE_AND_FWD_FLAG     STRING,\n",
    "  PULOCATIONID           NUMBER(38,0),\n",
    "  DOLOCATIONID           NUMBER(38,0),\n",
    "  PAYMENT_TYPE           NUMBER(38,0),\n",
    "  FARE_AMOUNT            FLOAT,\n",
    "  EXTRA                  FLOAT,\n",
    "  MTA_TAX                FLOAT,\n",
    "  TIP_AMOUNT             FLOAT,\n",
    "  TOLLS_AMOUNT           FLOAT,\n",
    "  IMPROVEMENT_SURCHARGE  FLOAT,\n",
    "  TOTAL_AMOUNT           FLOAT,\n",
    "  CONGESTION_SURCHARGE   FLOAT,\n",
    "  AIRPORT_FEE            FLOAT,          -- solo aplica a yellow\n",
    "  TRIP_TYPE              NUMBER(38,0),   -- solo aplica a green\n",
    "  CBD_CONGESTION_FEE     FLOAT,          -- recientes\n",
    "\n",
    "  _RUN_ID                STRING,\n",
    "  _INGESTED_AT           TIMESTAMP_NTZ,\n",
    "  _BATCH_TAG             STRING,\n",
    "  _CHUNK_ID              NUMBER(38,0)\n",
    ");\n",
    "\"\"\").collect()\n",
    "\n",
    "print(\"✅ Objetos listos:\")\n",
    "print(\"   Tabla RAW:\", TABLE)\n",
    "print(\"   Stage:\", f\"@{DB}.{SCHEMA}.{STAGE}\")\n",
    "print(\"   File Format:\", f\"{DB}.{SCHEMA}.{FF_PQ}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48899abd-f878-4140-959a-70f85250ecd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗓️ SERVICES=['green', 'yellow']  YEARS=2015-2025  MONTHS=01-12  RUN_ID=manual_0001\n"
     ]
    }
   ],
   "source": [
    "import sys, tempfile, requests\n",
    "from pathlib import Path\n",
    "\n",
    "# Parámetros del .env\n",
    "SERVICES = [\"green\", \"yellow\"]\n",
    "YEARS    = os.getenv(\"YEARS\", \"2019-2019\")\n",
    "MONTHS   = os.getenv(\"MONTHS\", \"01-03\")\n",
    "# SERVICES = [\"yellow\", \"green\"]\n",
    "# YEARS    = \"2015-2015\"\n",
    "# MONTHS   = \"01-01\"\n",
    "\n",
    "RUN_ID   = os.getenv(\"RUN_ID\", \"manual_0001\")\n",
    "\n",
    "# Descarga\n",
    "DOWNLOAD_TIMEOUT = int(os.getenv(\"DOWNLOAD_TIMEOUT\", \"120\"))\n",
    "RETRIES          = int(os.getenv(\"DOWNLOAD_RETRIES\", \"3\"))\n",
    "\n",
    "print(f\"🗓️ SERVICES={SERVICES}  YEARS={YEARS}  MONTHS={MONTHS}  RUN_ID={RUN_ID}\")\n",
    "\n",
    "def expand_range(rng: str, pad=False):\n",
    "    a,b = rng.split(\"-\")\n",
    "    xs = [str(x) for x in range(int(a), int(b)+1)]\n",
    "    return [s.zfill(2) for s in xs] if pad else xs\n",
    "\n",
    "YEARS_LIST  = expand_range(YEARS)\n",
    "MONTHS_LIST = expand_range(MONTHS, pad=True)\n",
    "\n",
    "def tlc_url(service, year, month, ext=\"parquet\"):\n",
    "    return f\"https://d37ci6vzurychx.cloudfront.net/trip-data/{service}_tripdata_{year}-{month}.{ext}\"\n",
    "\n",
    "def download_temp(url: str, suffix: str):\n",
    "    \"\"\"Descarga por streaming a archivo temporal efímero y devuelve su Path.\"\"\"\n",
    "    for attempt in range(1, RETRIES+1):\n",
    "        try:\n",
    "            r = requests.get(url, stream=True, timeout=DOWNLOAD_TIMEOUT)\n",
    "            if r.status_code != 200:\n",
    "                return None\n",
    "            total = int(r.headers.get(\"Content-Length\", 0))\n",
    "            done = 0; chunk = 1024*1024\n",
    "            fd, tmp_path = tempfile.mkstemp(suffix=suffix)\n",
    "            with os.fdopen(fd, \"wb\") as f:\n",
    "                for part in r.iter_content(chunk_size=chunk):\n",
    "                    if part:\n",
    "                        f.write(part); done += len(part)\n",
    "                        if total:\n",
    "                            pct = 100*done/max(total,1)\n",
    "                            sys.stdout.write(f\"\\r⬇️  {Path(tmp_path).name} {pct:5.1f}%\")\n",
    "                            sys.stdout.flush()\n",
    "            if total: sys.stdout.write(\"\\n\")\n",
    "            return Path(tmp_path)\n",
    "        except Exception as e:\n",
    "            print(f\"   intento {attempt}/{RETRIES} falló → {e}\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ec3d7da-cacb-4cf6-95bd-26274af6b7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_copy_select_sql(service: str, year: str, month: str, run_id: str) -> str:\n",
    "    db, sc, stg, ff, table = DB, SCHEMA, STAGE, FF_PQ, TABLE\n",
    "    y, m = int(year), int(month)\n",
    "\n",
    "    def fld(name: str) -> str:\n",
    "        # Devuelve un VARIANT del parquet con búsqueda case-insensitive\n",
    "        cand = [\n",
    "            f'$1:\"{name}\"',\n",
    "            f'$1:\"{name.lower()}\"',\n",
    "            f'$1:\"{name.upper()}\"',\n",
    "            f'$1:\"{name.capitalize()}\"',\n",
    "        ]\n",
    "        return \"COALESCE(\" + \", \".join(cand) + \")\"\n",
    "\n",
    "    def ts_auto(name: str) -> str:\n",
    "        v = fld(name)  # VARIANT\n",
    "        # 1) Si ya viene como TIMESTAMP_* en el VARIANT, castear directo a NTZ\n",
    "        # 2) Intentar parsear como string\n",
    "        # 3) Intentar como número (detectando escala por longitud)\n",
    "        num_expr = f\"TRY_TO_NUMBER(TO_VARCHAR({v}))\"\n",
    "        num_len  = f\"LENGTH(TO_VARCHAR({num_expr}))\"\n",
    "        return f\"\"\"\n",
    "        CASE\n",
    "          WHEN TYPEOF({v}) IN ('TIMESTAMP_NTZ','TIMESTAMP_LTZ','TIMESTAMP_TZ') THEN {v}::TIMESTAMP_NTZ\n",
    "          WHEN TRY_TO_TIMESTAMP_NTZ(TO_VARCHAR({v})) IS NOT NULL THEN TRY_TO_TIMESTAMP_NTZ(TO_VARCHAR({v}))\n",
    "          WHEN {num_expr} IS NOT NULL THEN\n",
    "            CASE\n",
    "              WHEN {num_len} >= 16 THEN TO_TIMESTAMP_NTZ({num_expr}, 6)  -- microsegundos\n",
    "              WHEN {num_len} >= 13 THEN TO_TIMESTAMP_NTZ({num_expr}, 3)  -- milisegundos\n",
    "              ELSE TO_TIMESTAMP_NTZ({num_expr}, 0)                       -- segundos\n",
    "            END\n",
    "          ELSE NULL\n",
    "        END\n",
    "        \"\"\"\n",
    "\n",
    "    # Campos comunes\n",
    "    vendorid              = f\"TRY_TO_NUMBER(TO_VARCHAR({fld('VendorID')}))\"\n",
    "    passenger_count       = f\"TRY_TO_NUMBER(TO_VARCHAR({fld('passenger_count')}))\"\n",
    "    trip_distance         = f\"TRY_TO_DOUBLE(TO_VARCHAR({fld('trip_distance')}))\"\n",
    "    ratecodeid            = f\"TRY_TO_NUMBER(TO_VARCHAR({fld('RatecodeID')}))\"\n",
    "    store_and_fwd_flag    = f\"TO_VARCHAR({fld('store_and_fwd_flag')})\"\n",
    "    pulocationid          = f\"TRY_TO_NUMBER(TO_VARCHAR({fld('PULocationID')}))\"\n",
    "    dolocationid          = f\"TRY_TO_NUMBER(TO_VARCHAR({fld('DOLocationID')}))\"\n",
    "    payment_type          = f\"TRY_TO_NUMBER(TO_VARCHAR({fld('payment_type')}))\"\n",
    "    fare_amount           = f\"TRY_TO_DOUBLE(TO_VARCHAR({fld('fare_amount')}))\"\n",
    "    extra                 = f\"TRY_TO_DOUBLE(TO_VARCHAR({fld('extra')}))\"\n",
    "    mta_tax               = f\"TRY_TO_DOUBLE(TO_VARCHAR({fld('mta_tax')}))\"\n",
    "    tip_amount            = f\"TRY_TO_DOUBLE(TO_VARCHAR({fld('tip_amount')}))\"\n",
    "    tolls_amount          = f\"TRY_TO_DOUBLE(TO_VARCHAR({fld('tolls_amount')}))\"\n",
    "    improvement_surcharge = f\"TRY_TO_DOUBLE(TO_VARCHAR({fld('improvement_surcharge')}))\"\n",
    "    total_amount          = f\"TRY_TO_DOUBLE(TO_VARCHAR({fld('total_amount')}))\"\n",
    "    congestion_surcharge  = f\"TRY_TO_DOUBLE(TO_VARCHAR({fld('congestion_surcharge')}))\"\n",
    "    airport_fee           = f\"TRY_TO_DOUBLE(TO_VARCHAR({fld('airport_fee')}))\"   # yellow\n",
    "    trip_type             = f\"TRY_TO_NUMBER(TO_VARCHAR({fld('trip_type')}))\"     # green\n",
    "    cbd_congestion_fee    = f\"TRY_TO_DOUBLE(TO_VARCHAR({fld('cbd_congestion_fee')}))\"\n",
    "\n",
    "    if service == \"yellow\":\n",
    "        pu_ts = ts_auto('tpep_pickup_datetime')\n",
    "        do_ts = ts_auto('tpep_dropoff_datetime')\n",
    "    elif service == \"green\":\n",
    "        pu_ts = ts_auto('lpep_pickup_datetime')\n",
    "        do_ts = ts_auto('lpep_dropoff_datetime')\n",
    "    else:\n",
    "        raise ValueError(\"service debe ser 'yellow' o 'green'\")\n",
    "\n",
    "    select_sql = f\"\"\"\n",
    "    SELECT\n",
    "      '{service}'::STRING                    AS SERVICE,\n",
    "      {y}::NUMBER                            AS YEAR,\n",
    "      {m}::NUMBER                            AS MONTH,\n",
    "\n",
    "      {vendorid}                             AS VENDORID,\n",
    "      {pu_ts}                                AS PICKUP_DATETIME,\n",
    "      {do_ts}                                AS DROPOFF_DATETIME,\n",
    "      {passenger_count}                      AS PASSENGER_COUNT,\n",
    "      {trip_distance}                        AS TRIP_DISTANCE,\n",
    "      {ratecodeid}                           AS RATECODEID,\n",
    "      {store_and_fwd_flag}                   AS STORE_AND_FWD_FLAG,\n",
    "      {pulocationid}                         AS PULOCATIONID,\n",
    "      {dolocationid}                         AS DOLOCATIONID,\n",
    "      {payment_type}                         AS PAYMENT_TYPE,\n",
    "      {fare_amount}                          AS FARE_AMOUNT,\n",
    "      {extra}                                AS EXTRA,\n",
    "      {mta_tax}                              AS MTA_TAX,\n",
    "      {tip_amount}                           AS TIP_AMOUNT,\n",
    "      {tolls_amount}                         AS TOLLS_AMOUNT,\n",
    "      {improvement_surcharge}                AS IMPROVEMENT_SURCHARGE,\n",
    "      {total_amount}                         AS TOTAL_AMOUNT,\n",
    "      {congestion_surcharge}                 AS CONGESTION_SURCHARGE,\n",
    "      {airport_fee}                          AS AIRPORT_FEE,\n",
    "      {trip_type}                            AS TRIP_TYPE,\n",
    "      {cbd_congestion_fee}                   AS CBD_CONGESTION_FEE,\n",
    "\n",
    "      '{run_id}'::STRING                     AS _RUN_ID,\n",
    "      CURRENT_TIMESTAMP()                    AS _INGESTED_AT,\n",
    "      '{service}/{y}/{str(m).zfill(2)}'      AS _BATCH_TAG,\n",
    "      NULL::NUMBER                           AS _CHUNK_ID\n",
    "    FROM @{db}.{sc}.{stg}/{service}/{y}/{str(m).zfill(2)}/\n",
    "      (FILE_FORMAT => {db}.{sc}.{ff})\n",
    "    \"\"\"\n",
    "\n",
    "    return f\"\"\"\n",
    "    COPY INTO {table}\n",
    "    FROM (\n",
    "      {select_sql}\n",
    "    )\n",
    "    ON_ERROR = 'ABORT_STATEMENT'\n",
    "    PURGE = FALSE\n",
    "    FORCE = TRUE;\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a92dfeb-65d5-4b32-b162-de9c0fd80044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_month(service: str, year: str, month: str, run_id: str) -> int:\n",
    "    print(f\"\\n📅 {service} {year}-{month} — inicio\")\n",
    "\n",
    "    # Descargar Parquet a temp\n",
    "    url = tlc_url(service, year, month, \"parquet\")\n",
    "    print(\"→ descargando:\", url)\n",
    "    tmp = download_temp(url, \".parquet\")\n",
    "    if tmp is None:\n",
    "        print(\"⚠️ no encontrado (parquet). Saltando.\")\n",
    "        return 0\n",
    "\n",
    "    try:\n",
    "        prefix = f\"@{DB}.{SCHEMA}.{STAGE}/{service}/{year}/{month}/\"\n",
    "\n",
    "        # PUT al stage (paralelo)\n",
    "        t_put = time.time()\n",
    "        put_res = session.file.put(\n",
    "            local_file_name=str(tmp),\n",
    "            stage_location=prefix,\n",
    "            auto_compress=False,\n",
    "            overwrite=True,\n",
    "            parallel=8\n",
    "        )\n",
    "        print(f\"   ⬆️ PUT ({len(put_res)} file/s) t={time.time()-t_put:,.1f}s\")\n",
    "\n",
    "        # Idempotencia: borra ese mes/servicio\n",
    "        session.sql(f\"\"\"\n",
    "          DELETE FROM {TABLE}\n",
    "          WHERE SERVICE='{service}' AND YEAR={int(year)} AND MONTH={int(month)}\n",
    "        \"\"\").collect()\n",
    "\n",
    "        # COPY SELECT (mapeo por data dictionary)\n",
    "        t_copy = time.time()\n",
    "        copy_sql = make_copy_select_sql(service, year, month, run_id)\n",
    "        res = session.sql(copy_sql).collect()\n",
    "        loaded = sum(r.asDict().get('rows_loaded', 0) for r in res)\n",
    "        print(f\"   ✅ COPY rows={loaded:,}  t={time.time()-t_copy:,.1f}s\")\n",
    "\n",
    "        # Limpieza del stage del mes (opcional pero recomendable)\n",
    "        session.sql(f\"REMOVE {prefix}\").collect()\n",
    "        print(\"🧽 Stage limpio para\", f\"{service}/{year}/{month}\")\n",
    "\n",
    "        print(f\"✅ {service} {year}-{month} — OK\")\n",
    "        return int(loaded)\n",
    "    finally:\n",
    "        try:\n",
    "            tmp.unlink(missing_ok=True)\n",
    "            print(f\"🧹 temp borrado: {tmp.name}\")\n",
    "        except Exception as e:\n",
    "            print(\"⚠️ no pude borrar temp:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35ab8bf-1f56-4e9c-8446-5cdd2292e511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📅 green 2015-01 — inicio\n",
      "→ descargando: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2015-01.parquet\n",
      "⬇️  tmpxem1_u0t.parquet 100.0%\n",
      "   ⬆️ PUT (1 file/s) t=3.9s\n",
      "   ✅ COPY rows=1,508,493  t=35.1s\n",
      "🧽 Stage limpio para green/2015/01\n",
      "✅ green 2015-01 — OK\n",
      "🧹 temp borrado: tmpxem1_u0t.parquet\n",
      "\n",
      "📅 green 2015-02 — inicio\n",
      "→ descargando: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2015-02.parquet\n",
      "⬇️  tmpew1txu93.parquet 100.0%\n",
      "   ⬆️ PUT (1 file/s) t=3.2s\n",
      "   ✅ COPY rows=1,574,830  t=35.7s\n",
      "🧽 Stage limpio para green/2015/02\n",
      "✅ green 2015-02 — OK\n",
      "🧹 temp borrado: tmpew1txu93.parquet\n",
      "\n",
      "📅 green 2015-03 — inicio\n",
      "→ descargando: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2015-03.parquet\n",
      "⬇️  tmprx0a44s8.parquet 100.0%\n",
      "   ⬆️ PUT (1 file/s) t=3.8s\n",
      "   ✅ COPY rows=1,722,574  t=39.3s\n",
      "🧽 Stage limpio para green/2015/03\n",
      "✅ green 2015-03 — OK\n",
      "🧹 temp borrado: tmprx0a44s8.parquet\n",
      "\n",
      "📅 green 2015-04 — inicio\n",
      "→ descargando: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2015-04.parquet\n",
      "⬇️  tmpxlmjcdnu.parquet 100.0%\n",
      "   ⬆️ PUT (1 file/s) t=3.6s\n",
      "   ✅ COPY rows=1,664,394  t=38.0s\n",
      "🧽 Stage limpio para green/2015/04\n",
      "✅ green 2015-04 — OK\n",
      "🧹 temp borrado: tmpxlmjcdnu.parquet\n",
      "\n",
      "📅 green 2015-05 — inicio\n",
      "→ descargando: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2015-05.parquet\n",
      "⬇️  tmpn_z9fied.parquet 100.0%\n",
      "   ⬆️ PUT (1 file/s) t=14.1s\n",
      "   ✅ COPY rows=1,786,848  t=40.7s\n",
      "🧽 Stage limpio para green/2015/05\n",
      "✅ green 2015-05 — OK\n",
      "🧹 temp borrado: tmpn_z9fied.parquet\n",
      "\n",
      "📅 green 2015-06 — inicio\n",
      "→ descargando: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2015-06.parquet\n",
      "⬇️  tmpqs13dazz.parquet 100.0%\n",
      "   ⬆️ PUT (1 file/s) t=3.2s\n",
      "   ✅ COPY rows=1,638,868  t=37.2s\n",
      "🧽 Stage limpio para green/2015/06\n",
      "✅ green 2015-06 — OK\n",
      "🧹 temp borrado: tmpqs13dazz.parquet\n",
      "\n",
      "📅 green 2015-07 — inicio\n",
      "→ descargando: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2015-07.parquet\n",
      "⬇️  tmp6voo2ir3.parquet 100.0%\n",
      "   ⬆️ PUT (1 file/s) t=3.6s\n",
      "   ✅ COPY rows=1,541,671  t=35.0s\n",
      "🧽 Stage limpio para green/2015/07\n",
      "✅ green 2015-07 — OK\n",
      "🧹 temp borrado: tmp6voo2ir3.parquet\n",
      "\n",
      "📅 green 2015-08 — inicio\n",
      "→ descargando: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2015-08.parquet\n",
      "⬇️  tmpqygk0_cg.parquet 100.0%\n",
      "   ⬆️ PUT (1 file/s) t=3.6s\n",
      "   ✅ COPY rows=1,532,343  t=35.0s\n",
      "🧽 Stage limpio para green/2015/08\n",
      "✅ green 2015-08 — OK\n",
      "🧹 temp borrado: tmpqygk0_cg.parquet\n",
      "\n",
      "📅 green 2015-09 — inicio\n",
      "→ descargando: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2015-09.parquet\n",
      "⬇️  tmpcm9f7nhd.parquet 100.0%\n",
      "   ⬆️ PUT (1 file/s) t=4.3s\n",
      "   ✅ COPY rows=1,494,927  t=33.9s\n",
      "🧽 Stage limpio para green/2015/09\n",
      "✅ green 2015-09 — OK\n",
      "🧹 temp borrado: tmpcm9f7nhd.parquet\n",
      "\n",
      "📅 green 2015-10 — inicio\n",
      "→ descargando: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2015-10.parquet\n",
      "⬇️  tmp1_tlm_kr.parquet 100.0%\n",
      "   ⬆️ PUT (1 file/s) t=13.2s\n",
      "   ✅ COPY rows=1,630,536  t=37.0s\n",
      "🧽 Stage limpio para green/2015/10\n",
      "✅ green 2015-10 — OK\n",
      "🧹 temp borrado: tmp1_tlm_kr.parquet\n",
      "\n",
      "📅 green 2015-11 — inicio\n",
      "→ descargando: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2015-11.parquet\n",
      "⬇️  tmp6x5zqxtt.parquet 100.0%\n",
      "   ⬆️ PUT (1 file/s) t=3.7s\n",
      "   ✅ COPY rows=1,529,984  t=34.2s\n",
      "🧽 Stage limpio para green/2015/11\n",
      "✅ green 2015-11 — OK\n",
      "🧹 temp borrado: tmp6x5zqxtt.parquet\n",
      "\n",
      "📅 green 2015-12 — inicio\n",
      "→ descargando: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2015-12.parquet\n",
      "⬇️  tmpuh21r47e.parquet 100.0%\n",
      "   ⬆️ PUT (1 file/s) t=3.4s\n",
      "   ✅ COPY rows=1,608,297  t=36.6s\n",
      "🧽 Stage limpio para green/2015/12\n",
      "✅ green 2015-12 — OK\n",
      "🧹 temp borrado: tmpuh21r47e.parquet\n",
      "\n",
      "📅 green 2016-01 — inicio\n",
      "→ descargando: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2016-01.parquet\n",
      "⬇️  tmp2pfz8q_g.parquet 100.0%\n",
      "   ⬆️ PUT (1 file/s) t=12.4s\n",
      "   ✅ COPY rows=1,445,292  t=33.1s\n",
      "🧽 Stage limpio para green/2016/01\n",
      "✅ green 2016-01 — OK\n",
      "🧹 temp borrado: tmp2pfz8q_g.parquet\n",
      "\n",
      "📅 green 2016-02 — inicio\n",
      "→ descargando: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2016-02.parquet\n",
      "⬇️  tmpymdsqyb2.parquet 100.0%\n",
      "   ⬆️ PUT (1 file/s) t=2.9s\n",
      "   ✅ COPY rows=1,510,722  t=34.2s\n",
      "🧽 Stage limpio para green/2016/02\n",
      "✅ green 2016-02 — OK\n",
      "🧹 temp borrado: tmpymdsqyb2.parquet\n",
      "\n",
      "📅 green 2016-03 — inicio\n",
      "→ descargando: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2016-03.parquet\n",
      "⬇️  tmpmy6771zq.parquet 100.0%\n",
      "   ⬆️ PUT (1 file/s) t=3.2s\n",
      "   ✅ COPY rows=1,576,393  t=36.5s\n",
      "🧽 Stage limpio para green/2016/03\n",
      "✅ green 2016-03 — OK\n",
      "🧹 temp borrado: tmpmy6771zq.parquet\n",
      "\n",
      "📅 green 2016-04 — inicio\n",
      "→ descargando: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2016-04.parquet\n",
      "⬇️  tmpgdau2w90.parquet 100.0%\n",
      "   ⬆️ PUT (1 file/s) t=3.0s\n",
      "   ✅ COPY rows=1,543,926  t=35.3s\n",
      "🧽 Stage limpio para green/2016/04\n",
      "✅ green 2016-04 — OK\n",
      "🧹 temp borrado: tmpgdau2w90.parquet\n",
      "\n",
      "📅 green 2016-05 — inicio\n",
      "→ descargando: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2016-05.parquet\n",
      "⬇️  tmp6vodewnn.parquet 100.0%\n",
      "   ⬆️ PUT (1 file/s) t=2.9s\n",
      "   ✅ COPY rows=1,536,979  t=35.2s\n",
      "🧽 Stage limpio para green/2016/05\n",
      "✅ green 2016-05 — OK\n",
      "🧹 temp borrado: tmp6vodewnn.parquet\n",
      "\n",
      "📅 green 2016-06 — inicio\n",
      "→ descargando: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2016-06.parquet\n",
      "⬇️  tmpyvpc4l3i.parquet 100.0%\n",
      "   ⬆️ PUT (1 file/s) t=4.3s\n",
      "   ✅ COPY rows=1,404,727  t=32.2s\n",
      "🧽 Stage limpio para green/2016/06\n",
      "✅ green 2016-06 — OK\n",
      "🧹 temp borrado: tmpyvpc4l3i.parquet\n",
      "\n",
      "📅 green 2016-07 — inicio\n",
      "→ descargando: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2016-07.parquet\n",
      "⬇️  tmpqak8nvqa.parquet 100.0%\n",
      "   ⬆️ PUT (1 file/s) t=3.1s\n",
      "   ✅ COPY rows=1,332,510  t=30.7s\n",
      "🧽 Stage limpio para green/2016/07\n",
      "✅ green 2016-07 — OK\n",
      "🧹 temp borrado: tmpqak8nvqa.parquet\n",
      "\n",
      "📅 green 2016-08 — inicio\n",
      "→ descargando: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2016-08.parquet\n",
      "⬇️  tmp33ugu82q.parquet 100.0%\n",
      "   ⬆️ PUT (1 file/s) t=3.5s\n",
      "   ✅ COPY rows=1,247,675  t=28.7s\n",
      "🧽 Stage limpio para green/2016/08\n",
      "✅ green 2016-08 — OK\n",
      "🧹 temp borrado: tmp33ugu82q.parquet\n",
      "\n",
      "📅 green 2016-09 — inicio\n",
      "→ descargando: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2016-09.parquet\n",
      "⬇️  tmpb1l4trwg.parquet 100.0%\n",
      "   ⬆️ PUT (1 file/s) t=2.8s\n",
      "   ✅ COPY rows=1,162,373  t=26.9s\n",
      "🧽 Stage limpio para green/2016/09\n",
      "✅ green 2016-09 — OK\n",
      "🧹 temp borrado: tmpb1l4trwg.parquet\n",
      "\n",
      "📅 green 2016-10 — inicio\n",
      "→ descargando: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2016-10.parquet\n",
      "⬇️  tmpslidtzhm.parquet 100.0%\n",
      "   ⬆️ PUT (1 file/s) t=2.9s\n",
      "   ✅ COPY rows=1,252,572  t=29.3s\n",
      "🧽 Stage limpio para green/2016/10\n",
      "✅ green 2016-10 — OK\n",
      "🧹 temp borrado: tmpslidtzhm.parquet\n",
      "\n",
      "📅 green 2016-11 — inicio\n",
      "→ descargando: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2016-11.parquet\n",
      "⬇️  tmpz7j5tr5s.parquet 100.0%\n",
      "   ⬆️ PUT (1 file/s) t=3.0s\n",
      "   ✅ COPY rows=1,148,214  t=26.5s\n",
      "🧽 Stage limpio para green/2016/11\n",
      "✅ green 2016-11 — OK\n",
      "🧹 temp borrado: tmpz7j5tr5s.parquet\n",
      "\n",
      "📅 green 2016-12 — inicio\n",
      "→ descargando: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2016-12.parquet\n",
      "⬇️  tmplxhgm75r.parquet 100.0%\n",
      "   ⬆️ PUT (1 file/s) t=3.0s\n",
      "   ✅ COPY rows=1,224,158  t=27.7s\n",
      "🧽 Stage limpio para green/2016/12\n",
      "✅ green 2016-12 — OK\n",
      "🧹 temp borrado: tmplxhgm75r.parquet\n",
      "\n",
      "📅 green 2017-01 — inicio\n",
      "→ descargando: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2017-01.parquet\n",
      "⬇️  tmp538a0zg2.parquet 100.0%\n",
      "   ⬆️ PUT (1 file/s) t=2.6s\n",
      "   ✅ COPY rows=1,069,565  t=24.5s\n",
      "🧽 Stage limpio para green/2017/01\n",
      "✅ green 2017-01 — OK\n",
      "🧹 temp borrado: tmp538a0zg2.parquet\n",
      "\n",
      "📅 green 2017-02 — inicio\n",
      "→ descargando: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2017-02.parquet\n",
      "⬇️  tmpngd1_8f7.parquet 100.0%\n",
      "   ⬆️ PUT (1 file/s) t=2.5s\n",
      "   ✅ COPY rows=1,022,313  t=23.2s\n",
      "🧽 Stage limpio para green/2017/02\n",
      "✅ green 2017-02 — OK\n",
      "🧹 temp borrado: tmpngd1_8f7.parquet\n",
      "\n",
      "📅 green 2017-03 — inicio\n",
      "→ descargando: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2017-03.parquet\n",
      "⬇️  tmpexarn4d7.parquet 100.0%\n",
      "   ⬆️ PUT (1 file/s) t=4.0s\n",
      "   ✅ COPY rows=1,157,827  t=26.6s\n",
      "🧽 Stage limpio para green/2017/03\n",
      "✅ green 2017-03 — OK\n",
      "🧹 temp borrado: tmpexarn4d7.parquet\n",
      "\n",
      "📅 green 2017-04 — inicio\n",
      "→ descargando: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2017-04.parquet\n",
      "⬇️  tmpfh7lddyo.parquet 100.0%\n",
      "   ⬆️ PUT (1 file/s) t=2.5s\n",
      "   ✅ COPY rows=1,080,844  t=25.1s\n",
      "🧽 Stage limpio para green/2017/04\n",
      "✅ green 2017-04 — OK\n",
      "🧹 temp borrado: tmpfh7lddyo.parquet\n",
      "\n",
      "📅 green 2017-05 — inicio\n",
      "→ descargando: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2017-05.parquet\n",
      "⬇️  tmpk2jrcybx.parquet 100.0%\n",
      "   ⬆️ PUT (1 file/s) t=2.5s\n",
      "   ✅ COPY rows=1,059,463  t=24.9s\n",
      "🧽 Stage limpio para green/2017/05\n",
      "✅ green 2017-05 — OK\n",
      "🧹 temp borrado: tmpk2jrcybx.parquet\n",
      "\n",
      "📅 green 2017-06 — inicio\n",
      "→ descargando: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2017-06.parquet\n",
      "⬇️  tmpce25io84.parquet 100.0%\n",
      "   ⬆️ PUT (1 file/s) t=2.8s\n",
      "   ✅ COPY rows=976,467  t=22.6s\n",
      "🧽 Stage limpio para green/2017/06\n",
      "✅ green 2017-06 — OK\n",
      "🧹 temp borrado: tmpce25io84.parquet\n",
      "\n",
      "📅 green 2017-07 — inicio\n",
      "→ descargando: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2017-07.parquet\n",
      "⬇️  tmpck4cyih1.parquet 100.0%\n",
      "   ⬆️ PUT (1 file/s) t=2.6s\n",
      "   ✅ COPY rows=914,783  t=20.8s\n",
      "🧽 Stage limpio para green/2017/07\n",
      "✅ green 2017-07 — OK\n",
      "🧹 temp borrado: tmpck4cyih1.parquet\n",
      "\n",
      "📅 green 2017-08 — inicio\n",
      "→ descargando: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2017-08.parquet\n",
      "⬇️  tmpk81whcq1.parquet 100.0%\n",
      "   ⬆️ PUT (1 file/s) t=3.5s\n",
      "   ✅ COPY rows=867,407  t=19.9s\n",
      "🧽 Stage limpio para green/2017/08\n",
      "✅ green 2017-08 — OK\n",
      "🧹 temp borrado: tmpk81whcq1.parquet\n",
      "\n",
      "📅 green 2017-09 — inicio\n",
      "→ descargando: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2017-09.parquet\n",
      "⬇️  tmpz8b377c3.parquet 100.0%\n",
      "   ⬆️ PUT (1 file/s) t=2.6s\n",
      "   ✅ COPY rows=882,464  t=20.3s\n",
      "🧽 Stage limpio para green/2017/09\n",
      "✅ green 2017-09 — OK\n",
      "🧹 temp borrado: tmpz8b377c3.parquet\n",
      "\n",
      "📅 green 2017-10 — inicio\n",
      "→ descargando: https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2017-10.parquet\n",
      "⬇️  tmp6l_4fe8a.parquet 100.0%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "summary = []\n",
    "t_all = time.time()\n",
    "for s in SERVICES:\n",
    "    for y in YEARS_LIST:\n",
    "        for m in MONTHS_LIST:\n",
    "            try:\n",
    "                n = ingest_month(s, y, m, RUN_ID)\n",
    "                summary.append((s, y, m, n))\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error en {s} {y}-{m} → {e}\")\n",
    "                summary.append((s, y, m, -1))\n",
    "\n",
    "print(f\"\\n🏁 Proceso completo en {time.time()-t_all:,.1f}s\")\n",
    "df = pd.DataFrame(summary, columns=[\"service\",\"year\",\"month\",\"rows_loaded\"])\n",
    "display(df.tail(12))\n",
    "\n",
    "from pathlib import Path\n",
    "EVID = Path(\"/home/jovyan/work/evidence\"); EVID.mkdir(parents=True, exist_ok=True)\n",
    "out = EVID / f\"raw_ingest_summary_{RUN_ID}.csv\"\n",
    "df.to_csv(out, index=False)\n",
    "print(\"📄 evidencia:\", out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb5a86a-b1a3-46c7-b525-12ba30ad816d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
