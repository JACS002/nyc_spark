# üöå Proyecto 03 ‚Äì Data Mining 2025  
### Universidad San Francisco de Quito  
**Curso:** Data Mining  
**Estudiante:** Joel Cuascota
**Fecha:** Octubre 2025  

---

## üß† Resumen
Este proyecto replica un flujo completo de *Data Engineering + Analytics* sobre el dataset **NYC TLC Trips (Yellow & Green, 2015‚Äì2025)**.  
La infraestructura se levanta con **Docker Compose** (servicio √∫nico `spark-notebook`) y el destino anal√≠tico es **Snowflake**, organizado en dos esquemas:

- **RAW:** aterrizaje espejo de Parquet con metadatos de ingesta.  
- **ANALYTICS:** tabla unificada `analytics.obt_trips` (modelo *One Big Table*, OBT).

Se procesan, validan y analizan los datos mediante notebooks en Jupyter/Spark o Snowpark Python.

---

## üéØ Objetivos de aprendizaje
- Operar Spark en Jupyter para ingesta masiva y transformaci√≥n ligera de Parquet.  
- Dise√±ar un aterrizaje RAW y una OBT desnormalizada para anal√≠tica directa.  
- Practicar un modelo alternativo al dimensional (One Big Table).  
- Gestionar seguridad y reproducibilidad con Docker y variables de ambiente.  
- Implementar controles de calidad, idempotencia y auditor√≠a de cargas.


## üèóÔ∏è Arquitectura general

```text
Parquet (NYC TLC 2015‚Äì2025)
   ‚îÇ
   ‚ñº
Spark (Jupyter Notebook en contenedor)
   ‚îÇ
   ‚îú‚îÄ‚îÄ Ingesta RAW ‚Üí Snowflake.RAW
   ‚îú‚îÄ‚îÄ Enriquecimiento/Unificaci√≥n (zones, cat√°logos)
   ‚îú‚îÄ‚îÄ Construcci√≥n OBT (derivadas + metadatos)
   ‚îî‚îÄ‚îÄ Validaciones y An√°lisis (Snowpark)
         ‚ñº
         Snowflake.ANALYTICS.OBT_TRIPS
```

## üß∞ Herramientas clave

- **Docker + Docker Compose**  
- **Jupyter + PySpark**  
- **Snowflake (con Snowpark Python)**  
- **Pandas + Matplotlib** para visualizaciones  

---

## ‚öôÔ∏è Variables de ambiente (`.env`)

Todas las credenciales y par√°metros se manejan mediante un archivo `.env`  
*(sin credenciales reales en GitHub)*.

### Ejemplo de `.env.example`
```bash
SNOWFLAKE_ACCOUNT=xxxxxx
SNOWFLAKE_USER=xxxxx
SNOWFLAKE_PASSWORD=xxxxx
SNOWFLAKE_ROLE=ACCOUNTADMIN
SNOWFLAKE_WAREHOUSE=COMPUTE_WH
SNOWFLAKE_DATABASE=NYC_TAXI_DM
SNOWFLAKE_SCHEMA_RAW=RAW
SNOWFLAKE_SCHEMA_ANALYTICS=ANALYTICS
PARQUET_PATH=/data/parquet
RUN_ID=P3_$(date +%Y%m%d_%H%M)
```

## üß© Notebooks y prop√≥sito

| Notebook | Prop√≥sito principal |
|-----------|--------------------|
| **01_ingesta_parquet_raw.ipynb** | Lee Parquet (Yellow/Green) 2015‚Äì2025 y carga a RAW. |
| **02_enriquecimiento_y_unificacion.ipynb** | Integra cat√°logos (zones, vendor, rate, payment). |
| **03_construccion_obt.ipynb** | Construye `analytics.obt_trips` (derivadas, idempotencia). |
| **04_validaciones_y_exploracion.ipynb** | Valida nulos, rangos, coherencia, conteos. |
| **05_data_analysis.ipynb** | Responde 20 preguntas de negocio con `analytics.obt_trips`. |


## üóìÔ∏è Cobertura procesada

La siguiente matriz resume la cobertura temporal y el estado de procesamiento por servicio.  
Los resultados completos se documentan en el archivo CSV de evidencia:

üìÑ **`evidence/matriz_cobertura.csv`**

## üì¶ Dise√±o de esquemas

### üóÇÔ∏è RAW
- **Grano:** viaje original.  
- **Metadatos:** `_run_id`, `_ingested_at_utc`, `service`, `year`, `month`.  
- **Uso:** staging y auditor√≠a.  

---

### üßÆ ANALYTICS.OBT_TRIPS (One Big Table)
- **Grano:** 1 fila = 1 viaje.  
- **Derivadas:**
  - `trip_duration_min = datediff('minute', pickup, dropoff)`  
  - `avg_speed_mph = distance / (duration/60)`  
  - `tip_pct = tip_amount / fare_amount`  
- **Metadatos:** `run_id`, `built_at_utc`, `source_service`, `source_year`, `source_month`.  
- **Idempotencia:** control mediante `TRIP_ID = HASH(...)`.

---

## ‚úÖ Calidad y auditor√≠a

**Validaciones ejecutadas:**
- Nulos en campos esenciales (`pickup`, `dropoff`, `location`, `payment`) ‚Üí ‚úÖ 0 nulos.  
- **Rangos l√≥gicos:**
  - Duraci√≥n: 0‚Äì48h  
  - Distancia: 0‚Äì150mi  
  - Velocidad ‚â§ 100 mph ‚Üí ‚úÖ sin fuera de rango.  
- **Coherencia temporal:** `dropoff ‚â• pickup` ‚Üí ‚úÖ.  
- **Conteos por servicio/mes:** reproducen los conteos originales (12.7M yellow, 1.5M green).  

**Auditor√≠a:**  
Archivo `validacion_obt_quality_summary.csv` con m√©tricas por servicio.


## üìä Resultados anal√≠ticos (enero 2015)

| Indicador | Hallazgo |
|------------|-----------|
| **Top zonas pickup/dropoff** | Manhattan (UES, Midtown, Times Sq) dominan. |
| **Ticket promedio** | \$15 (yellow), \$14.7 (green). |
| **Tip promedio** | ~14% en tarjeta. |
| **Duraci√≥n p50/p90** | 10 / 22 min en Manhattan. |
| **Velocidad promedio** | 12‚Äì13 mph, cae en hora pico. |
| **RateCode top** | Standard, JFK, Newark. |
| **Mix service** | 96% yellow en Manhattan, 70% green en Brooklyn. |
| **M√©todos de pago** | 60% tarjeta, 39% efectivo. |
| **YoY** | Sin datos previos (solo enero 2015). |

---

## üß± Ejecuci√≥n paso a paso

1. Clonar el repositorio y crear el archivo `.env` (basado en `.env.example`).  
2. Levantar el entorno con Docker Compose:  
   ```bash
   docker compose up -d
   ```
3. Acceder a Jupyter: http://localhost:8888
4. Ejecutar los notebooks en orden: 01 ‚Üí 05
5. Revisar los outputs en las carpetas: evidence/


## üóÇÔ∏è Carpeta de evidencias (`/evidence`)

| Evidencia               | Descripci√≥n esperada                                               |
|-------------------------|-------------------------------------------------------------------|
| `docker_running.png`     | `docker ps` mostrando contenedor `spark-notebook` activo.        |
| `jupyter_home.png`       | Vista de JupyterLab con notebooks visibles.                      |
| `spark_ui.png`           | Spark UI (puerto 4040) ejecutando tareas.                        |
| `raw_counts.png`         | Conteos por servicio/a√±o/mes despu√©s de ingesta RAW.             |
| `obt_summary.png`        | Conteo total en `analytics.obt_trips` (12.7M yellow, 1.5M green). |
| `validaciones.png`       | Resultados de `04_validaciones_y_exploracion.ipynb`.             |
| `analysis_outputs.png`   | Muestra de resultados (top zonas, payment, vendor, etc.).        |
| `snowflake_console.png`  | Vista de las tablas RAW y OBT en Snowflake.                      |


## üßæ Checklist de aceptaci√≥n

- Docker Compose levanta Spark + Jupyter
- Variables desde `.env`
- Carga completa 2015‚Äì2025 (al menos validado 2015‚Äì01)
- `analytics.obt_trips` creada con derivadas y metadatos
- Idempotencia verificada (reingesta 2015‚Äì01)
- Validaciones completas (rangos, nulos, coherencia)
- 20 preguntas analizadas
- README con pasos y evidencias

---

## üìà Conclusiones

- Se logr√≥ una OBT robusta, idempotente y sin duplicados.
- Los resultados anal√≠ticos confirman patrones hist√≥ricos de tr√°fico en NYC.
- Snowpark result√≥ m√°s simple y eficiente que Spark puro para an√°lisis SQL.
- Infraestructura reproducible v√≠a Docker y variables de entorno.
